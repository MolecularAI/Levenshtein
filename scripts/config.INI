# File to the scripts (run directory for slurm jobs)
script_path="/home/kfxl284/git/levenshteinaugment/scripts"
# Datafile used in 01_split.py
datafile="/home/kfxl284/git/reaction_dataset_creation/Create_splits/small_dataset.csv.gz"
# Path where the temporary files will be created and stored (n_partitions * number_augmentations files)
scratch_path="/scratch/kfxl284/data_splits_levenshtein/"
# Path for final collection of augmentations by 04_copy_from_scratch.sh
target_path="/projects/mai/users/kfxl284_esben/temp/levenshtein_test"

#Conda environment details
conda_activate="/home/kfxl284/miniconda3/bin/activate"
conda_env="/projects/mai/users/kfxl284_esben/envs/levensthein_augment_env"

#Number of datachuncks to work on
n_partitions=500

#Input columns are random selections of augmentations and output is aligned.
separator="\t"
input_column="reactants"
output_column="products"

#Randomizations to build pool to select from
randomization_tries=500
#Number of final augmented datafiles
number_augmentations=200



