# File to the scripts (run directory for slurm jobs)
script_path="/path/to/levenshtein/scripts"

# Datafile used in 01_split.py
datafile="/path/to/datafile.csv.zip"
# Path where the temporary files will be created and stored (n_partitions * number_augmentations files)
scratch_path="/scratch/USERID/data_splits/"
# Path for final collection of augmentations by 04_copy_from_scratch.sh
target_path="/path/to/targetdir/levenshtein"


#Conda environment details
conda_activate="/path/to/miniconda3/bin/activate"
conda_env="/path/to/envs/levensthein_augment_env"

#Number of datachuncks to work on
n_partitions=4096

#Input columns are random selections of augmentations and output is aligned. Seperator is the seperator of the CSV
separator="\t"
input_column="reactants"
output_column="products"

#Randomizations to build pool to select from
randomization_tries=5000
#Number of final augmented datafiles
number_augmentations=200



